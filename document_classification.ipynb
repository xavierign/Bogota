{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data/SDQS 2017-enejunV2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tokenize documents\n",
    "\n",
    "from collections import namedtuple\n",
    "from nltk.stem import  SnowballStemmer\n",
    " \n",
    "stemmer = SnowballStemmer('spanish')\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(df['Asunto']):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "    \n",
    "docs_w2v = []\n",
    "\n",
    "for i, text in enumerate(df['Asunto']):\n",
    "    words = text.lower().split()\n",
    "    words_stemm = [stemmer.stem(i) for i in words]\n",
    "    docs_w2v.append(words_stemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#apply doc2vec\n",
    "\n",
    "from gensim.models import doc2vec,word2vec\n",
    "model = word2vec.Word2Vec(docs_w2v, size = 100, window = 300, min_count = 10, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a vector of average the words.\n",
    "average_vect =[]\n",
    "\n",
    "for sen in docs_w2v:\n",
    "    \n",
    "    ave = np.zeros(100)\n",
    "    for word in sen:\n",
    "        try:\n",
    "            ave = ave + model[word]/len(sen)\n",
    "        except:\n",
    "            pass\n",
    "    average_vect.append(ave)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find distances between tickets\n",
    "X = np.array(average_vect)\n",
    "#s = pd.Series(df['Sector'])\n",
    "#Y = pd.factorize(s)[0]\n",
    "Y = pd.get_dummies(df['Sector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#replace low freq# run if needed\n",
    "\n",
    "df_temp = pd.DataFrame(Y)\n",
    "df_temp2 = df_temp.apply(lambda x: x.mask(x.map(x.value_counts())<4500, 999) if x.name!='0' else x)\n",
    "Y = df_temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train, y_test = train_test_split(X,Y,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-merror:0.412723\ttest-merror:0.425921\n",
      "[1]\ttrain-merror:0.375083\ttest-merror:0.393068\n",
      "[2]\ttrain-merror:0.353765\ttest-merror:0.376846\n",
      "[3]\ttrain-merror:0.338996\ttest-merror:0.36753\n",
      "[4]\ttrain-merror:0.328529\ttest-merror:0.358323\n",
      "Test error using softmax = 0.3583230579531443\n"
     ]
    }
   ],
   "source": [
    "#classification\n",
    "import xgboost as xgb\n",
    "xg_train = xgb.DMatrix(X_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['num_class'] = len(pd.factorize(s)[1])\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_test, 'test')]\n",
    "num_round = 5\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist)\n",
    "# get prediction\n",
    "pred = bst.predict(xg_test)\n",
    "error_rate = np.sum(pred != y_test) / y_test.shape[0]\n",
    "print('Test error using softmax = {}'.format(error_rate))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## keras deep learning ##\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model.add(Dense(units=64, input_dim=100))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(units=16))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "109485/109485 [==============================] - 8s - loss: 1.0316 - acc: 0.6954     \n",
      "Epoch 2/15\n",
      "109485/109485 [==============================] - 8s - loss: 1.0198 - acc: 0.6995     \n",
      "Epoch 3/15\n",
      "109485/109485 [==============================] - 8s - loss: 1.0083 - acc: 0.7032     \n",
      "Epoch 4/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9980 - acc: 0.7044     \n",
      "Epoch 5/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9886 - acc: 0.7067     \n",
      "Epoch 6/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9790 - acc: 0.7084     \n",
      "Epoch 7/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9692 - acc: 0.7108     \n",
      "Epoch 8/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9614 - acc: 0.7129     \n",
      "Epoch 9/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9539 - acc: 0.7148     \n",
      "Epoch 10/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9460 - acc: 0.7154     \n",
      "Epoch 11/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9388 - acc: 0.7179     \n",
      "Epoch 12/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9327 - acc: 0.7188     \n",
      "Epoch 13/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9256 - acc: 0.7212     \n",
      "Epoch 14/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9193 - acc: 0.7236     \n",
      "Epoch 15/15\n",
      "109485/109485 [==============================] - 8s - loss: 0.9133 - acc: 0.7249     \n",
      "34688/36495 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
    "model.fit(np.array(X_train), np.array(y_train), epochs=15, batch_size=32)\n",
    "#model.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "\n",
    "#Evaluate your performance in one line:\n",
    "loss_and_metrics = model.evaluate(np.array(X_test), np.array(y_test), batch_size=128)\n",
    "\n",
    "#Or generate predictions on new data:\n",
    "classes = model.predict(np.array(X_test), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.70957665433621042"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.argmax(classes, axis=1)==np.argmax(np.array(y_test), axis=1))/len(np.argmax(classes, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### keras with iris###\n",
    "type(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.csv\") #, header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=4, activation='relu'))\n",
    "\tmodel.add(Dense(3, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### wordtovec with subtema.\n",
    "###Â NOT WORKING ###\n",
    "\n",
    "from collections import namedtuple\n",
    "from nltk.stem import  SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "#word to vec with subtema\n",
    "Subtema = np.unique(df['Subtema'])\n",
    "\n",
    "#replace low freq# run if needed\n",
    "#df_temp = pd.DataFrame(Subtema)\n",
    "#df_temp2 = df_temp.apply(lambda x: x.mask(x.map(x.value_counts())<15, 'OTROS') if x.name!='0' else x)\n",
    "#Y = df_temp2\n",
    "\n",
    "docs_w2v=[]\n",
    "\n",
    "for i, text in enumerate(Subtema):\n",
    "    words = text.lower().split()\n",
    "    words_stemm = [stemmer.stem(j) for j in words]\n",
    "    docs_w2v.append(words_stemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec,word2vec\n",
    "model = word2vec.Word2Vec(docs_w2v, size = 200, window = 300, min_count = 1, workers = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## biuld the average by row\n",
    "average_vect =[]\n",
    "for sen in docs_w2v:\n",
    "    \n",
    "    ave = np.zeros(200)\n",
    "    for word in sen:\n",
    "\n",
    "        ave = ave + model[word]/len(sen)        \n",
    "    average_vect.append(ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "mat = np.matrix(average_vect)\n",
    "labels = SpectralClustering(100).fit_predict(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "### For the purposes of this example, we store feature data from our\n",
    "### dataframe `df`, in the `f1` and `f2` arrays. We combine this into\n",
    "### a feature matrix `X` before entering it into the algorithm.\n",
    "#f1 = df['Distance_Feature'].values\n",
    "#f2 = df['Speeding_Feature'].values\n",
    "\n",
    "X=np.matrix(average_vect)\n",
    "kmeans = KMeans(n_clusters=100).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = {'subtema': Subtema, 'cluster':labels}\n",
    "pd.DataFrame(data).to_csv('subtema.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
